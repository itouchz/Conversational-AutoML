{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFERENCES\n",
    "\n",
    "**Information Extraction**\n",
    "- https://www.analyticsvidhya.com/blog/2020/06/nlp-project-information-extraction/\n",
    "- https://medium.com/analytics-vidhya/introduction-to-information-extraction-using-python-and-spacy-858f5d6416ca\n",
    "\n",
    "**Chatbot**\n",
    "- https://medium.com/predict/create-your-chatbot-using-python-nltk-761cd0aeaed3\n",
    "- https://medium.com/swlh/a-chatbot-in-python-using-nltk-938a37a9eacc\n",
    "\n",
    "**Intent**\n",
    "- https://medium.com/walmartglobaltech/joint-intent-classification-and-entity-recognition-for-conversational-commerce-35bf69195176\n",
    "- https://medium.com/analytics-vidhya/machine-learning-intent-classification-221ecded7c74\n",
    "- https://colab.research.google.com/github/deepmipt/dp_notebooks/blob/master/DP_autoFAQ.ipynb (!)\n",
    "- https://towardsdatascience.com/a-brief-introduction-to-intent-classification-96fda6b1f557\n",
    "- https://medium.com/artefact-engineering-and-data-science/nlu-benchmark-for-intent-detection-and-named-entity-recognition-in-call-center-conversations-f58e5b4c8d3d\n",
    "- https://medium.com/iambot/ai-assistance-with-pytext-6308d896566d\n",
    "\n",
    "**NER**\n",
    "路 Simple Entities\n",
    "路 Composite Entities\n",
    "路 Entity Roles\n",
    "路 Entity Lists\n",
    "路 Regular Expressions\n",
    "路 Prebuilt Models\n",
    "- https://github.com/DhruvilKarani/NER-Blog/blob/master/analysis.ipynb\n",
    "- https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "- https://towardsdatascience.com/named-entity-recognition-ner-using-keras-bidirectional-lstm-28cd3f301f54\n",
    "- https://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede\n",
    "- https://towardsdatascience.com/deep-learning-for-named-entity-recognition-3-reusing-a-bidirectional-lstm-cnn-on-clinical-text-e84bd28052df\n",
    "- https://medium.com/@b.terryjack/nlp-pretrained-named-entity-recognition-7caa5cd28d7b\n",
    "- https://medium.com/swlh/custom-natural-language-processing-831a9a8d3dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import wordnet  # to perform lemmitization\n",
    "from nltk import pos_tag  # for parts of speech\n",
    "from nltk import word_tokenize  # to create tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # intent + entity type\n",
    "# intent_sets = ['greet', 'new', 'update', 'finish', 'query', 'no']\n",
    "# # possible data types: tabular, image, and text (?) -- training and testing set for each\n",
    "# slot_sets = {\n",
    "#     \"task\": [], # regression or classification\n",
    "#     \"data_source\": [], # upload, url, or built-in\n",
    "#     \"target_variable\": [], # specific name or undefined\n",
    "#     \"dataset\": [], # dataset name or filepath\n",
    "#     \"delivery\": [], # on-web or email\n",
    "# }\n",
    "\n",
    "# # from user query sentences\n",
    "# constructed_pipeline = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # possible tasks: tabular classification, tabular regression, image classification, image regression, text classification, and text regression\n",
    "# states = ['standby', 'inquire', 'inference', 'running', 'deliver'] # possible states of the CA and AutoML Engine\n",
    "# user_slot = {\"method\": None, \"task\": None, \"data_source\": None, \"dataset\": None, \"target\": None, \"delivery\": 'chat'}\n",
    "\n",
    "# def intent_classification():\n",
    "#     return intent\n",
    "\n",
    "# error handling\n",
    "# generatin data set (may use dictionary based synonym replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of available Datasets (built-in or .csv, .txt, .xls, folders with .png or .jpg), Algorithms (just simply ML or DL), and Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple keyword matching?\n",
    "# Rule-based grammar matching\t\n",
    "\n",
    "def text_normalization(text):\n",
    "#     text=str(text).lower() # text to lower case\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # stop_words.add('please')\n",
    "    text = re.sub('\\-', '', text)\n",
    "    text = re.sub('[^a-zA-z0-9\\_]', ' ', text) # removing special characters\n",
    "    text = nltk.word_tokenize(text) # word tokenizing\n",
    "    lema = wordnet.WordNetLemmatizer() # intializing lemmatization\n",
    "    tags_list = pos_tag(text, tagset=None) # parts of speech\n",
    "    lema_words = []   # empty list \n",
    "    for token, pos_token in tags_list:\n",
    "        if pos_token.startswith('V'):  # Verb\n",
    "            pos_val = 'v'\n",
    "        elif pos_token.startswith('J'): # Adjective\n",
    "            pos_val = 'a'\n",
    "        elif pos_token.startswith('R'): # Adverb\n",
    "            pos_val = 'r'\n",
    "        else:\n",
    "            pos_val = 'n' # Noun\n",
    "        lema_token = lema.lemmatize(token, pos_val) # performing lemmatization\n",
    "        lema_words.append(lema_token) # appending the lemmatized token into a list\n",
    "    \n",
    "    # lema_words = pos_tag(lema_words)\n",
    "    # text = [item for item in lema_words if item[0].lower() not in stop_words]\n",
    "    lema_words = [item for item in lema_words if item.lower() not in ['a', 'an', 'the']]\n",
    "    return \" \".join(lema_words) # returns the lemmatized tokens as a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GREETING_INPUTS = [\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\", \"morning\", \"afternoon\", \"evening\", \"night\"]\n",
    "CONFIRM_WORDS = [\"yes\", \"yep\", \"okay\", \"ok\", \"sure\", \"certainly\", \"definitely\", \"absolutely\", \"go ahead\", \"cool\", \"right\", \"of course\"]\n",
    "DENY_WORDS = [\"no\", \"nope\", \"na\", \"not yet\", \"not sure\", \"more\", \"not\", \"don't\", \"do not\", \"again\"]\n",
    "END_WORDS = [\"goodbye\", \"bye\", \"see you later\", \"end\", \"finish\", \"stop\", \"not anymore\"]\n",
    "ML_METHODS = ['machine learning', 'ml', 'machine']\n",
    "DL_METHODS = ['deep learning', 'dl', 'neural network', 'nn', 'neural', 'network']\n",
    "CLASSIFICATIONS = ['class', 'classify', 'classification', 'classifier', 'discrete output']\n",
    "REGRESSIONS = ['regress', 'regression', 'regressor', 'continuous output']\n",
    "DATA_SOURCES = ['upload', '<url>', 'this data', 'this dataset', 'my data', 'my dataset']\n",
    "IMAGE_TYPES = ['image', 'picture', 'figure', 'art', 'draw', 'photo', 'photograph', 'portrait', 'painting', 'visual', 'illustration', 'symbol', 'view', 'vision', 'sketch', 'icon']\n",
    "TEXT_TYPES = ['text', 'word', 'message', 'writing', 'script', 'content', 'document', 'passage', 'context', 'essay', 'manuscript', 'paper', 'language', 'letter', 'written', 'write', 'character', 'note', 'darft']\n",
    "TABLE_TYPES = ['structured', 'structure', 'tabular', 'table', 'relation', 'database', 'dataframe', 'frame', 'normal', 'excel', 'csv', 'file', 'summary', 'process']\n",
    "# AVAIL_DATASETS = pd.read_csv('openml_datasets.csv')['name'].apply(lambda x: x.lower()).to_list()\n",
    "AVAIL_DATASETS = tfds.list_builders()\n",
    "TARGET_VAR = ['be dependent variable', 'be target variable', 'be dependent feature' 'be target feature', 'variable be', 'feature be', 'value', 'feature', 'variable', 'predict', 'forecast', 'classify'] # regex ?\n",
    "DELIVERY = ['by email', 'by e-mail', 'email', 'e-mail']\n",
    "\n",
    "GREETING_RESPONSES = [\"Yep, It's nice to see you here! \", \"Hey~\", \"*nods*\", \"Hi there!\", \"Hello\", \"I am glad! You are talking to me~~~\"]\n",
    "END_RESPONSES = [\"See you then! \", \"Bye~\", \"Goodluck!\", \"Hope to see you again~\", \"Goodbye!~\", \"Thanks~\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_slot = {\"method\": None, \"task\": None, \"data_source\": None,\n",
    "             \"data_type\": None, \"dataset\": None, \"target\": None, \"delivery\": 'chat'}\n",
    "\n",
    "\n",
    "def reset_slot():\n",
    "    global user_slot\n",
    "    user_slot = {\"method\": None, \"task\": None, \"data_source\": None,\n",
    "                 \"data_type\": None, \"dataset\": None, \"target\": None, \"delivery\": 'chat'}\n",
    "\n",
    "\n",
    "def is_slot_complete():\n",
    "    return user_slot['method'] != None and user_slot['task'] != None and user_slot['data_source'] != None and user_slot['data_type'] != None and user_slot['dataset'] != None and user_slot['target'] != None and user_slot['delivery'] != None\n",
    "\n",
    "\n",
    "def response_for_incomplete_slot(slot):\n",
    "    return ''\n",
    "\n",
    "\n",
    "def update_slot(msg):\n",
    "    global user_slot\n",
    "\n",
    "    for ml in ML_METHODS:\n",
    "        if ml in msg:\n",
    "            user_slot['method'] = 'ml'\n",
    "            break\n",
    "\n",
    "    for dl in DL_METHODS:\n",
    "        if dl in msg:\n",
    "            user_slot['method'] = 'dl'\n",
    "            break\n",
    "\n",
    "    for img in IMAGE_TYPES:\n",
    "        if img in msg:\n",
    "            user_slot['data_type'] = 'image'\n",
    "            break\n",
    "\n",
    "    for txt in TEXT_TYPES:\n",
    "        if txt in msg:\n",
    "            user_slot['data_type'] = 'text'\n",
    "            break\n",
    "\n",
    "    for table in TABLE_TYPES:\n",
    "        if table in msg:\n",
    "            user_slot['data_type'] = 'table'\n",
    "            break\n",
    "\n",
    "    for cl in CLASSIFICATIONS:\n",
    "        if cl in msg:\n",
    "            user_slot['task'] = 'cls'\n",
    "            break\n",
    "\n",
    "    for reg in REGRESSIONS:\n",
    "        if reg in msg:\n",
    "            user_slot['task'] = 'reg'\n",
    "            break\n",
    "\n",
    "    for ds in AVAIL_DATASETS:\n",
    "        if ds in msg:\n",
    "            user_slot['dataset'] = ds\n",
    "            user_slot['data_source'] = 'built_in'\n",
    "            user_slot['target'] = 'label'\n",
    "\n",
    "    for ds in DATA_SOURCES:\n",
    "        if ds in msg:\n",
    "            user_slot['data_source'] = 'user_define'\n",
    "\n",
    "    for d in DELIVERY:\n",
    "        if d in msg:\n",
    "            user_slot['delivery'] = 'email'\n",
    "            break\n",
    "\n",
    "    for tv in TARGET_VAR:\n",
    "        if tv in msg:\n",
    "            if tv in ['value', 'feature', 'variable', 'be target variable', 'be target feature']:\n",
    "                user_slot['target'] = msg.split(tv)[0].split()[-1]\n",
    "            else:\n",
    "                user_slot['target'] = msg.split(tv)[-1].split()[0]\n",
    "\n",
    "\n",
    "def standby_state(user_message):\n",
    "    text = ''\n",
    "    msg = user_message.lower()\n",
    "    current_state = 'standby'\n",
    "    global user_slot\n",
    "\n",
    "    for greet in GREETING_INPUTS:\n",
    "        if greet in msg:\n",
    "            text += random.choice(GREETING_RESPONSES) + '<br/>'\n",
    "            break\n",
    "\n",
    "    for end_word in END_WORDS:\n",
    "        if end_word in msg:\n",
    "            current_state = 'end'\n",
    "            text = random.choice(END_RESPONSES)\n",
    "            break\n",
    "\n",
    "    if current_state != 'end':\n",
    "        update_slot(msg)\n",
    "        if is_slot_complete():\n",
    "            text += f\"All you requested are well received! <br/> Please review the following list, do you want to proceed? <br/>\"\n",
    "            current_state = 'await'\n",
    "        else:\n",
    "            if user_slot['data_source'] == 'user_define' and user_slot['data_source'] == None:\n",
    "                text += 'Please upload your data file (.csv, .txt, .zip) below.'\n",
    "\n",
    "            current_state = 'active'\n",
    "    else:\n",
    "        current_state = 'standby'\n",
    "\n",
    "    return text, current_state, user_slot\n",
    "\n",
    "\n",
    "def active_state(user_message, await_feature=None):\n",
    "    text = ''\n",
    "    msg = user_message.lower()\n",
    "    current_state = 'active'\n",
    "    global user_slot\n",
    "\n",
    "    for greet in GREETING_INPUTS:\n",
    "        if greet in msg:\n",
    "            text += random.choice(GREETING_RESPONSES) + ' (again) <br/>'\n",
    "            break\n",
    "\n",
    "    for end_word in END_WORDS:\n",
    "        if end_word in msg:\n",
    "            current_state = 'standby'\n",
    "            text = random.choice(END_RESPONSES)\n",
    "            break\n",
    "\n",
    "    if current_state != 'standby':\n",
    "        if await_feature != None:\n",
    "            pass\n",
    "        else:\n",
    "            update_slot(msg)\n",
    "\n",
    "        if is_slot_complete():\n",
    "            text += f\"All you requested are well received! <br/> Please reiview the model's specs, do you would like to proceed? <br/>\"\n",
    "            current_state = 'await'\n",
    "        else:\n",
    "            if user_slot['data_source'] == 'user_define' and user_slot['data_source'] == None:\n",
    "                text += 'Please upload your data file (.csv, .txt, .zip) below.'\n",
    "\n",
    "            current_state = 'active'\n",
    "\n",
    "    return text, current_state, user_slot\n",
    "\n",
    "\n",
    "def await_state(user_message):\n",
    "    text = ''\n",
    "    msg = user_message.lower()\n",
    "    current_state = 'await'\n",
    "    global user_slot\n",
    "\n",
    "    for greet in GREETING_INPUTS:\n",
    "        if greet in msg:\n",
    "            text += 'Still wanna greet now?,  <br/>'\n",
    "            break\n",
    "\n",
    "    for end_word in END_WORDS:\n",
    "        if end_word in msg:\n",
    "            current_state = 'standby'\n",
    "            text = random.choice(END_RESPONSES)\n",
    "            break\n",
    "\n",
    "    for con in CONFIRM_WORDS:\n",
    "        if con in msg:\n",
    "            current_state = 'building'\n",
    "            break\n",
    "\n",
    "    for den in DENY_WORDS:\n",
    "        if den in msg:\n",
    "            current_state = 'await'\n",
    "            text += ' Umm... Please check your requirement~ <br/>'\n",
    "            break\n",
    "\n",
    "    if current_state != 'building' and current_state != 'standby':\n",
    "        if is_slot_complete():\n",
    "            text += f\"I've got all needed information as follows. <br/> Do you want to proceed? </br>\"\n",
    "            current_state = 'await'\n",
    "\n",
    "    return text, current_state, user_slot\n",
    "\n",
    "\n",
    "def building_state(user_message):\n",
    "    text = ''\n",
    "    current_state = 'building'\n",
    "    global user_slot\n",
    "\n",
    "    return text, current_state, user_slot\n",
    "\n",
    "\n",
    "def get_response(current_state, user_message, await_feature):\n",
    "    filtered_text = text_normalization(user_message)\n",
    "    await_feature = await_feature\n",
    "\n",
    "    response = {\n",
    "        'standby': standby_state(filtered_text),\n",
    "        'active': active_state(filtered_text, await_feature),\n",
    "        'await': await_state(filtered_text),\n",
    "        'building': building_state(filtered_text),\n",
    "    }\n",
    "\n",
    "    return response[current_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bot: Hi ! I'm your model builder烩~ Just tell me which model do you want by simply following the examples below.\n",
      "User: Hi\n",
      "Bot: Hi there!<br/> active\n",
      "User: I want deep learning model for image classification with MNIST dataset\n",
      "Bot: All you requested are well received! <br/> Please reiview the model's specs, do you would like to proceed? <br/> await\n",
      "User: yes\n",
      "Bot:  building\n"
     ]
    }
   ],
   "source": [
    "current_state = 'standby'\n",
    "print(\"Bot:\", \"Hi ! I'm your model builder烩~ Just tell me which model do you want by simply following the examples below.\")\n",
    "\n",
    "while current_state != 'building':\n",
    "    user_query = input()\n",
    "    print('User:', text_normalization(user_query))\n",
    "    response, current_state, user_slots = get_response(current_state, user_query, None)\n",
    "    print('Bot:', response, current_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import autokeras as ak\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import zipfile\n",
    "import autosklearn\n",
    "import autosklearn.classification\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "from sklearn.utils.multiclass import type_of_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_slot = {'method': 'dl', 'task': 'cls', 'data_source': 'built_in', 'data_type': 'image', 'dataset': 'mnist', 'target': 'label', 'delivery': 'chat'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trial 2 Complete [01h 27m 41s]\n",
      "val_accuracy: 0.3147590458393097\n",
      "\n",
      "Best val_accuracy So Far: 0.6897590160369873\n",
      "Total elapsed time: 01h 28m 23s\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "image_block_1/b...|efficient         |vanilla           \n",
      "image_block_1/n...|True              |True              \n",
      "image_block_1/a...|True              |False             \n",
      "image_block_1/i...|True              |None              \n",
      "image_block_1/i...|False             |None              \n",
      "image_block_1/i...|0                 |None              \n",
      "image_block_1/i...|0                 |None              \n",
      "image_block_1/i...|0.1               |None              \n",
      "image_block_1/i...|0                 |None              \n",
      "image_block_1/e...|True              |None              \n",
      "image_block_1/e...|b7                |None              \n",
      "image_block_1/e...|True              |None              \n",
      "image_block_1/e...|True              |None              \n",
      "classification_...|global_avg        |flatten           \n",
      "classification_...|0                 |0.5               \n",
      "optimizer         |adam              |adam              \n",
      "learning_rate     |2e-05             |0.001             \n",
      "\n",
      "Epoch 1/3\n",
      " 17/198 [=>............................] - ETA: 3:39:06 - loss: 2.3186 - accuracy: 0.1268"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8d57def1e832>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_slot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl/lib/python3.7/site-packages/autokeras/tasks/image.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, epochs, callbacks, validation_split, validation_data, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         )\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl/lib/python3.7/site-packages/autokeras/auto_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, callbacks, validation_split, validation_data, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         )\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl/lib/python3.7/site-packages/autokeras/engine/tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, epochs, callbacks, validation_split, **fit_kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# Train the best model use validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl/lib/python3.7/site-packages/kerastuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl/lib/python3.7/site-packages/kerastuner/engine/tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'callbacks'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl/lib/python3.7/site-packages/autokeras/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, fit_args, fit_kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         _, history = utils.fit_with_adaptive_batch_size(\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         )\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl/lib/python3.7/site-packages/autokeras/utils/utils.py\u001b[0m in \u001b[0;36mfit_with_adaptive_batch_size\u001b[0;34m(model, batch_size, **fit_kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfit_with_adaptive_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     history = run_with_adaptive_batch_size(\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     )\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl/lib/python3.7/site-packages/autokeras/utils/utils.py\u001b[0m in \u001b[0;36mrun_with_adaptive_batch_size\u001b[0;34m(batch_size, func, **fit_kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResourceExhaustedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl/lib/python3.7/site-packages/autokeras/utils/utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfit_with_adaptive_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     history = run_with_adaptive_batch_size(\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     )\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/automl/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if user_slot['data_source'] == 'built_in':\n",
    "    train_ds = tfds.load(user_slot['dataset'], split='train[:10%]+test[-10%:]', as_supervised=True, shuffle_files=True)\n",
    "else:\n",
    "    if user_slot['data_type'] == 'image':\n",
    "        # unzip --> train, test folders\n",
    "        with zipfile.ZipFile(f\"./upload/{user_slot['dataset']}\", 'r') as zip_ref:\n",
    "            zip_ref.extractall('./dataset/')\n",
    "    elif user_slot['data_type'] == 'text':\n",
    "        # .csv or .txt\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "        # .csv or .txt\n",
    "\n",
    "if user_slot['method'] == 'dl':\n",
    "    if user_slot['task'] == 'cls':\n",
    "        if user_slot['data_type'] == 'image':\n",
    "            clf = ak.ImageClassifier(overwrite=True, max_trials=3, objective='val_accuracy')\n",
    "            clf.fit(train_ds, epochs=3, validation_split=0.10)\n",
    "            model = clf.export_model()\n",
    "            model.save(\"model.h5\")\n",
    "        elif user_slot['data_type'] == 'text':\n",
    "\n",
    "    else:\n",
    "        if user_slot['data_type'] == 'image':\n",
    "            reg = ak.ImageRegressor(overwrite=True, max_trials=3)\n",
    "            reg.fit(train_ds, epochs=3, validation_split=0.10)\n",
    "            model = reg.export_model()\n",
    "            model.save(\"model.h5\")\n",
    "        elif user_slot['data_type'] == 'text':\n",
    "\n",
    "else:\n",
    "    pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "f265702b3991ac2f040706eb093507229943e0e8c0e79846a46d3504a0a7c00c"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}