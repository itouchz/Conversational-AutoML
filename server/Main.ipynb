{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFERENCES\n",
    "\n",
    "**Information Extraction**\n",
    "- https://www.analyticsvidhya.com/blog/2020/06/nlp-project-information-extraction/\n",
    "- https://medium.com/analytics-vidhya/introduction-to-information-extraction-using-python-and-spacy-858f5d6416ca\n",
    "\n",
    "**Chatbot**\n",
    "- https://medium.com/predict/create-your-chatbot-using-python-nltk-761cd0aeaed3\n",
    "- https://medium.com/swlh/a-chatbot-in-python-using-nltk-938a37a9eacc\n",
    "\n",
    "**Intent**\n",
    "- https://medium.com/walmartglobaltech/joint-intent-classification-and-entity-recognition-for-conversational-commerce-35bf69195176\n",
    "- https://medium.com/analytics-vidhya/machine-learning-intent-classification-221ecded7c74\n",
    "- https://colab.research.google.com/github/deepmipt/dp_notebooks/blob/master/DP_autoFAQ.ipynb (!)\n",
    "- https://towardsdatascience.com/a-brief-introduction-to-intent-classification-96fda6b1f557\n",
    "- https://medium.com/artefact-engineering-and-data-science/nlu-benchmark-for-intent-detection-and-named-entity-recognition-in-call-center-conversations-f58e5b4c8d3d\n",
    "- https://medium.com/iambot/ai-assistance-with-pytext-6308d896566d\n",
    "\n",
    "**NER**\n",
    "· Simple Entities\n",
    "· Composite Entities\n",
    "· Entity Roles\n",
    "· Entity Lists\n",
    "· Regular Expressions\n",
    "· Prebuilt Models\n",
    "- https://github.com/DhruvilKarani/NER-Blog/blob/master/analysis.ipynb\n",
    "- https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "- https://towardsdatascience.com/named-entity-recognition-ner-using-keras-bidirectional-lstm-28cd3f301f54\n",
    "- https://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede\n",
    "- https://towardsdatascience.com/deep-learning-for-named-entity-recognition-3-reusing-a-bidirectional-lstm-cnn-on-clinical-text-e84bd28052df\n",
    "- https://medium.com/@b.terryjack/nlp-pretrained-named-entity-recognition-7caa5cd28d7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/Users/itouchz.me/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\n  File \"/Users/itouchz.me/anaconda3/lib/python3.7/imp.py\", line 296, in find_module\n    raise ImportError(_ERR_MSG.format(name), name=name)\nImportError: No module named '_pywrap_tensorflow_internal'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/itouchz.me/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/Users/itouchz.me/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/Users/itouchz.me/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\n    import _pywrap_tensorflow_internal\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/imp.py\u001b[0m in \u001b[0;36mfind_module\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ERR_MSG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named '_pywrap_tensorflow_internal'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_pywrap_tensorflow_internal'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4a7299e9d050>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/Users/itouchz.me/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\n  File \"/Users/itouchz.me/anaconda3/lib/python3.7/imp.py\", line 296, in find_module\n    raise ImportError(_ERR_MSG.format(name), name=name)\nImportError: No module named '_pywrap_tensorflow_internal'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/itouchz.me/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/Users/itouchz.me/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/Users/itouchz.me/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\n    import _pywrap_tensorflow_internal\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import wordnet, PorterStemmer # to perform lemmitization\n",
    "from nltk.corpus import stopwords # for stop words\n",
    "from nltk import pos_tag # for parts of speech\n",
    "from nltk import word_tokenize # to create tokens\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer # to perform bow\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # to perform tfidf\n",
    "from sklearn.metrics import pairwise_distances # to perfrom cosine similarity\n",
    "\n",
    "# from sklearn.datasets import fetch_openml\n",
    "# import stanfordnlp\n",
    "# stanfordnlp.download('en') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intent + entity type\n",
    "intent_sets = ['greet', 'new', 'update', 'finish', 'query', 'no']\n",
    "# possible data types: tabular, image, and text (?) -- training and testing set for each\n",
    "slot_sets = {\n",
    "    \"task\": [], # regression or classification\n",
    "    \"data_source\": [], # upload, url, or built-in\n",
    "    \"target_variable\": [], # specific name or undefined\n",
    "    \"dataset\": [], # dataset name or filepath\n",
    "    \"delivery\": [], # on-web or email\n",
    "}\n",
    "\n",
    "# from user query sentences\n",
    "constructed_pipeline = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible tasks: tabular classification, tabular regression, image classification, image regression, text classification, and text regression\n",
    "states = ['standby', 'inquire', 'inference', 'running', 'deliver'] # possible states of the CA and AutoML Engine\n",
    "user_slot = {\"method\": None, \"task\": None, \"data_source\": None, \"dataset\": None, \"target\": None, \"delivery\": 'chat'}\n",
    "\n",
    "def intent_classification():\n",
    "    return intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "    'start': [],\n",
    "    'ongoing': [],\n",
    "    'interrupt': [],\n",
    "    'end': [],\n",
    "}\n",
    "\n",
    "# error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generatin data set (may use dictionary based synonym replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple keyword matching?\n",
    "# Rule-based grammar matching\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of available Datasets (built-in or .csv, .txt, .xls, folders with .png or .jpg), Algorithms (just simply ML or DL), and Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalization(text):\n",
    "#     text=str(text).lower() # text to lower case\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('please')\n",
    "    text = re.sub('[^a-zA-z0-9]', ' ', text) # removing special characters\n",
    "    text = nltk.word_tokenize(text) # word tokenizing\n",
    "    lema = wordnet.WordNetLemmatizer() # intializing lemmatization\n",
    "    tags_list = pos_tag(text, tagset=None) # parts of speech\n",
    "    lema_words = []   # empty list \n",
    "    for token, pos_token in tags_list:\n",
    "        if pos_token.startswith('V'):  # Verb\n",
    "            pos_val = 'v'\n",
    "        elif pos_token.startswith('J'): # Adjective\n",
    "            pos_val = 'a'\n",
    "        elif pos_token.startswith('R'): # Adverb\n",
    "            pos_val = 'r'\n",
    "        else:\n",
    "            pos_val = 'n' # Noun\n",
    "        lema_token = lema.lemmatize(token, pos_val) # performing lemmatization\n",
    "        lema_words.append(lema_token) # appending the lemmatized token into a list\n",
    "    \n",
    "    lema_words = pos_tag(lema_words)\n",
    "    text = [item for item in lema_words if item[0].lower() not in stop_words]\n",
    "    return text # returns the lemmatized tokens as a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = 'Hi! Please build an ML model that can classify 10 digits with MNIST data. Thanks~'\n",
    "\n",
    "GREETING_INPUTS = [\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\", \"morning\", \"afternoon\", \"evening\", \"night\"]\n",
    "CONFIRM_WORDS = [\"yes\", \"yep\", \"okay\", \"ok\", \"sure\", \"certainly\", \"definitely\", \"absolutely\", \"go ahead\", \"cool\", \"right\", \"of course\"]\n",
    "DENY_WORDS = [\"no\", \"nope\", \"na\", \"not yet\", \"not sure\", \"more\", \"not\", \"don't\", \"do not\", \"again\"]\n",
    "END_WORDS = [\"goodbye\", \"bye\", \"see you later\", \"thanks\", \"thank\", \"thank you\"]\n",
    "ML_METHODS = ['machine learning', 'ml', 'machine']\n",
    "DL_METHODS = ['deep learning', 'dl', 'neural network', 'nn', 'neural', 'network']\n",
    "CLASSIFICATIONS = ['class', 'classify', 'classification', 'classifier', 'discrete output']\n",
    "REGRESSIONS = ['regress', 'regression', 'regressor', 'continuous output']\n",
    "DATA_SOURCES = ['upload', '<url>', 'this data', 'this dataset', 'my data', 'my dataset']\n",
    "IMAGE_TYPES = ['image', 'picture', 'figure', 'art', 'draw', 'photo', 'photograph', 'portrait', 'painting', 'visual', 'illustration', 'symbol', 'view', 'vision', 'sketch', 'icon']\n",
    "TEXT_TYPES = ['text', 'word', 'message', 'writing', 'script', 'content', 'document', 'passage', 'context', 'essay', 'manuscript', 'paper', 'language', 'letter', 'written', 'write', 'character', 'note', 'darft']\n",
    "TABLE_TYPES = ['structured', 'structure', 'tabular', 'table', 'relation', 'database', 'dataframe', 'frame', 'normal', 'excel', 'csv', 'file', 'summary', 'process']\n",
    "# AVAIL_DATASETS = pd.read_csv('openml_datasets.csv')['name'].apply(lambda x: x.lower()).to_list()\n",
    "AVAIL_DATASETS = tfds.list_builders()\n",
    "TARGET_VAR = ['<name> value', 'predict <name>', 'forecast <name>', 'classify <name>'] # regex ?\n",
    "DELIVERY = ['by email', 'by e-mail', 'email', 'e-mail']\n",
    "\n",
    "GREETING_RESPONSES = [\"Yep, It's nice to see you here! 🙌🏻\", \"Hey~\", \"*nods*\", \"Hi there!\", \"Hello\", \"I am glad! You are talking to me~~~\"]\n",
    "END_RESPONSES = [\"See you then! 🙌🏻\", \"Bye~\", \"Goodluck!\", \"Hope to see you again~\", \"Goodbye!~\", \"Thanks~\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_slot = {\"method\": None, \"task\": None, \"data_source\": None, \"data_type\": None, \"dataset\": None, \"target\": None, \"delivery\": 'chat'}\n",
    "\n",
    "def standby_state(user_message):\n",
    "    text = ''\n",
    "    global user_slot\n",
    "    \n",
    "    for message in user_message:\n",
    "        if message.lower() in GREETING_INPUTS:\n",
    "            text += random.choice(GREETING_RESPONSES) + '\\n'\n",
    "        elif message.lower() in END_WORDS:\n",
    "            current_state = 'end'\n",
    "            text = random.choice(END_RESPONSES)\n",
    "        elif message.lower() in ML_METHODS:\n",
    "            user_slot['method'] = 'ml'\n",
    "        elif message.lower() in DL_METHODS:\n",
    "            user_slot['method'] = 'dl'\n",
    "        elif message.lower() in IMAGE_TYPES:\n",
    "            user_slot['data_type'] = 'image'\n",
    "        elif message.lower() in TEXT_TYPES:\n",
    "            user_slot['data_type'] = 'text'\n",
    "        elif message.lower() in TABLE_TYPES:\n",
    "            user_slot['data_type'] = 'table'\n",
    "        elif message.lower() in CLASSIFICATIONS:\n",
    "            user_slot['task'] = 'cls'\n",
    "        elif message.lower() in REGRESSIONS:\n",
    "            user_slot['task'] = 'reg'\n",
    "        elif message.lower() in AVAIL_DATASETS:\n",
    "            user_slot['dataset'] = message.lower()\n",
    "            user_slot['data_source'] = 'built_in'\n",
    "            user_slot['target'] = 'label'\n",
    "            tfds.load(message.lower())\n",
    "        elif message.lower() in DATA_SOURCES:\n",
    "            user_slot['data_source'] = 'user_define'\n",
    "            user_slot['dataset'] = 'filepath'\n",
    "        elif message.lower() in DELIVERY:\n",
    "            user_slot['delivery'] = 'email'\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    if current_state != 'end':\n",
    "        if user_slot['method'] != None and user_slot['task'] != None and user_slot['data_source'] != None and user_slot['dataset'] != None and user_slot['target'] != None and user_slot['delivery'] != None:\n",
    "            text += f\"All you requested are well received! \\n These are your requirements: {user_slot['method']}, {user_slot['task']}, {user_slot['dataset']}, {user_slot['target']}, and {user_slot['delivery']} \\n Do you want to proceed?\"\n",
    "            current_state = 'await'\n",
    "\n",
    "        else:\n",
    "            text += 'I need more things to complete. Please specify the following list:\\n'\n",
    "            for key, value in user_slot.items():\n",
    "                if value == None:\n",
    "                    text += '- ' + re.sub('_', ' ', key) + '\\n'\n",
    "\n",
    "            current_state = 'active'\n",
    "    else:\n",
    "        current_state = 'standby'\n",
    "        \n",
    "    return text, current_state, user_slot\n",
    "\n",
    "\n",
    "def active_state(user_message):\n",
    "    text = ''\n",
    "    global user_slot\n",
    "\n",
    "    for message in user_message:\n",
    "        if message.lower() in GREETING_INPUTS:\n",
    "            text += random.choice([\"Yep! We've already greeted!\", \"I've already known you~\", \"Please go to the next step!\"]) + '\\n'\n",
    "        elif message.lower() in END_WORDS:\n",
    "            current_state = 'standby'\n",
    "            text = random.choice(END_RESPONSES)\n",
    "        elif message.lower() in ML_METHODS:\n",
    "            user_slot['method'] = 'ml'\n",
    "        elif message.lower() in DL_METHODS:\n",
    "            user_slot['method'] = 'dl'\n",
    "        elif message.lower() in IMAGE_TYPES:\n",
    "            user_slot['data_type'] = 'image'\n",
    "        elif message.lower() in TEXT_TYPES:\n",
    "            user_slot['data_type'] = 'text'\n",
    "        elif message.lower() in TABLE_TYPES:\n",
    "            user_slot['data_type'] = 'table'\n",
    "        elif message.lower() in CLASSIFICATIONS:\n",
    "            user_slot['task'] = 'cls'\n",
    "        elif message.lower() in REGRESSIONS:\n",
    "            user_slot['task'] = 'reg'\n",
    "        elif message.lower() in AVAIL_DATASETS:\n",
    "            user_slot['dataset'] = message.lower()\n",
    "            user_slot['data_source'] = 'built_in'\n",
    "            user_slot['target'] = 'label'\n",
    "            tfds.load(message.lower())\n",
    "        elif message.lower() in DATA_SOURCES:\n",
    "            user_slot['data_source'] = 'user_define'\n",
    "            user_slot['dataset'] = 'filepath'\n",
    "        elif message.lower() in DELIVERY:\n",
    "            user_slot['delivery'] = 'email'\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    if current_state != 'standby':\n",
    "        if user_slot['method'] != None and user_slot['task'] != None and user_slot['data_source'] != None and user_slot['dataset'] != None and user_slot['target'] != None and user_slot['delivery'] != None:\n",
    "            text += f\"All you requested are well received! \\n These are your requirements: {user_slot['method']}, {user_slot['task']}, {user_slot['dataset']}, {user_slot['target']}, and {user_slot['delivery']} \\n Do you want to proceed?\"\n",
    "            current_state = 'await'\n",
    "        else:\n",
    "            text += 'I need more things to complete. Please specify the following list:\\n'\n",
    "            for key, value in user_slot.items():\n",
    "                if value == None:\n",
    "                    text += '- ' + re.sub('_', ' ', key) + '\\n'\n",
    "            current_state = 'active'\n",
    "\n",
    "    return text, current_state, user_slot\n",
    "\n",
    "def await_state(user_message):\n",
    "    text = ''\n",
    "    current_state = 'await'\n",
    "    global user_slot\n",
    "    \n",
    "    for message in user_message:\n",
    "        if message.lower() in GREETING_INPUTS:\n",
    "            text += random.choice([\"Yep! We've already greeted!\", \"I've already known you~\", \"Please go to the next step!\"]) + '\\n'\n",
    "        elif message.lower() in END_WORDS:\n",
    "            current_state = 'standby'\n",
    "            text = random.choice(END_RESPONSES)\n",
    "        elif message.lower() in CONFIRM_WORDS:\n",
    "            current_state = 'building'\n",
    "        elif message.lower() in DENY_WORDS:\n",
    "            current_state = 'await'\n",
    "            text += 'Umm... Please check your requirement~'\n",
    "        elif message.lower() in ML_METHODS:\n",
    "            user_slot['method'] = 'ml'\n",
    "        elif message.lower() in DL_METHODS:\n",
    "            user_slot['method'] = 'dl'\n",
    "        elif message.lower() in IMAGE_TYPES:\n",
    "            user_slot['data_type'] = 'image'\n",
    "        elif message.lower() in TEXT_TYPES:\n",
    "            user_slot['data_type'] = 'text'\n",
    "        elif message.lower() in TABLE_TYPES:\n",
    "            user_slot['data_type'] = 'table'\n",
    "        elif message.lower() in CLASSIFICATIONS:\n",
    "            user_slot['task'] = 'cls'\n",
    "        elif message.lower() in REGRESSIONS:\n",
    "            user_slot['task'] = 'reg'\n",
    "        elif message.lower() in AVAIL_DATASETS:\n",
    "            user_slot['dataset'] = message.lower()\n",
    "            user_slot['data_source'] = 'built_in'\n",
    "            user_slot['target'] = 'label'\n",
    "            tfds.load(message.lower())\n",
    "        elif message.lower() in DATA_SOURCES:\n",
    "            user_slot['data_source'] = 'user_define'\n",
    "            user_slot['dataset'] = 'filepath'\n",
    "        elif message.lower() in DELIVERY:\n",
    "            user_slot['delivery'] = 'email'\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    if current_state != 'building' and current_state != 'standby':   \n",
    "        if user_slot['method'] != None and user_slot['task'] != None and user_slot['data_source'] != None and user_slot['dataset'] != None and user_slot['target'] != None and user_slot['delivery'] != None:\n",
    "            text += f\"All you requested are well received! \\n These are your requirements: {user_slot['method']}, {user_slot['task']}, {user_slot['dataset']}, {user_slot['target']}, and {user_slot['delivery']} \\n Do you want to proceed?\"\n",
    "            current_state = 'await'\n",
    "        \n",
    "    return text, current_state, user_slot\n",
    "\n",
    "def building_state(user_message):\n",
    "    text = ''\n",
    "    global user_slot\n",
    "\n",
    "    return text, current_state, user_slot\n",
    "\n",
    "def response_text(current_state, user_message):    \n",
    "    filtered_text = [token[0] for token in text_normalization(user_message)]\n",
    "\n",
    "    response = {\n",
    "        'standby': standby_state(filtered_text),\n",
    "        'active': active_state(filtered_text),\n",
    "        'await': await_state(filtered_text),\n",
    "        'building': building_state(filtered_text),\n",
    "    }\n",
    "    \n",
    "    return \"I am sorry. I don't understand you. Please refer to the following examples.\" if response[current_state]  == '' else response[current_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state = 'standby'\n",
    "print(\"Bot:\", \"Hi 👋🏻! I'm your model builder🧑🏻‍💻~ Just tell me which model do you want by simply following the examples below👇🏻.\")\n",
    "\n",
    "while True:\n",
    "    user_query = input()\n",
    "    response, current_state, user_slots = response_text(current_state, user_query)\n",
    "    print(response, current_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import autokeras as ak\n",
    "import autosklearn as ask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}